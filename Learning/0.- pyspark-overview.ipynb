{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between;width:100%; height:120px;align-items: center; padding:20px; box-sizing:border-box;\">\n",
    "    <h1 style=\"margin:0;padding:0;\">Spark</h1>\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f3/Apache_Spark_logo.svg\" \n",
    "alt=\"\" style=\"height:110px;margin:0;passing:0;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Big Data** is the set of storage and processing methodologies. No doupt The `Hadoop` and `Spark` projects are linked to Big Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributed Computing Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Big Data, Distibuted Computing, and Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The search engine providers like **Google** and **Yahoo!** were the first group of users faced with\n",
    "Internet scale problems, mainly how to process and store indexes of all the documents in the Internet universe.\n",
    "\n",
    "* The whitepapers (Map reduce and The Google System) released by Google inspired to *Cutting*, who is working in a web crawler project, to apply the concepts of Map Reduce to his project, The resulting product is what we know today as\n",
    "**Hadoop**.\n",
    "\n",
    "* Other technologies and facts also emerged:\n",
    "  * The expansion of the ecommerce\n",
    "  * The birth and rapid growth of the internet mobile\n",
    "  * Social media \n",
    "\n",
    "* Those led to an exponential increase in the amount of data generate and the need to move to big data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. *Data Warehousing*: the practice of storing together a large amount of data from different parts of a business\n",
    "\n",
    "2. *The Apache Hadoop* software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hadoop Explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop is a data storage and processing *platform* initially based on a central concept: **data locality**. \n",
    "> *Data locality* \n",
    ">\n",
    "> Refers to the pattern of processing data where it resides *by bringing the computation (what we want to compute) to the data* rather than the typical pattern of requesting data from its location and sending the data to a remote processing system or host.\n",
    "> Recall, the data is distributed in clusters, those clusters can perform computing \n",
    "\n",
    "> *Platform* \n",
    ">\n",
    "> In computer systems, a **framework** is often a layered structure indicating what kind of programs can or should be built and how they would interrelate. Some computer system frameworks also include actual programs, specify programming interfaces, or offer programming tools for using the frameworks. Source: [Techtarget.com](https://www.techtarget.com/whatis/definition/framework#:~:text=In%20computer%20systems%2C%20a%20framework,tools%20for%20using%20the%20frameworks.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hadoop enables large datasets to be processed locally on the nodes of a cluster using a **shared nothing approach**, where each node can independently process a much smaller subset of the entire dataset without needing to communicate with other nodes. This characteristic is enabled through its implementation of a distributed filesystem\n",
    "\n",
    "* Hadoop is **schemaless with respect to its write operations** meaning when we write the data into a file in Hadoop, there is no indexes, no metadata captured about the type of data written into the hdfs (Hadoop Distributed File System) Hadoop platform.\n",
    "\n",
    "* Hadoop catchs the schema of the input data, that is, when only it is reading. This is reason Haddop is **Schema-on-read**.\n",
    "\n",
    "* This means Hadoop can store and process a wide range of data, from unstructured text documents, to semi-structured JSON (JavaScript Object Notation) or XML documents, to well-structured extracts from relational database systems.\n",
    "\n",
    "* Hadoop divides and overcome large problems into sets of smaller problems and applying the concepts of data locality and shared nothing, that we explained above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core Components of Hadoop\n",
    "Hadoop has two core components: HDFS (Hadoop Distributed File System) and YARN (Yet Another Resource Negotiator).\n",
    "\n",
    "* **HDFS** is Hadoop’s **storage** subsystem.\n",
    "* **YARN** can be thought of as Hadoop’s **processing**, or resource scheduling, subsystem.\n",
    "<center>\n",
    "<img src = \"./assets/img/Captura_hadoop.png\", height = 300, >\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each *component* is **independent** of the other and can operate in its own cluster.\n",
    "However, when a HDFS cluster and a YARN cluster are collocated with each\n",
    "other, the combination of both systems is considered to be a Hadoop cluster\n",
    "\n",
    "> **Cluster**\n",
    ">\n",
    "> A cluster is a collection of systems that work together to perform functions, such as computational or processing functions. *Individual servers* within a cluster are referred to as *nodes*. Clusters can have many topologies (structures) and communication models; one such model is the **master/slave model**. Master/slave is a model of communication whereby one process has control over one or more other processes.\n",
    "> The node that take the role of  master can be predefined.\n",
    "\n",
    "<center>\n",
    "<img src = \"./assets/img/Captura_hadoop_02.png\", height = 300>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any other projects that interact or integrate with Hadoop are called Hadoop `ecosystem` projects. For instance **Pig**\n",
    "\n",
    "You could consider `Spark` an ecosystem project, but this is debatable because **Spark does not require Hadoop to run**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src = \"./assets/img/hadoop-ecosystem.png\", height = 400>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDFS: Files, Blocks, and Metadata\n",
    "\n",
    "A *node* can see like a *individual server*, this node can perform functions like or master or slave. On the other hand, the node can be part of or HDFS or YARN cluster.\n",
    "The process in which the node become in node that store data to perform function like slave(which perform the computing) is called *DataNode*.\n",
    "\n",
    "HDFS is a *virtual filesystem* where files are composed of blocks distributed across one or more nodes of the *cluster HDFS*. \n",
    "\n",
    "Files are split  according to a configured *block size* upon uploading data into the filesystem, in a process known as *ingestion*.\n",
    "\n",
    "The **blocks are then distributed and replicated across cluster nodes to achieve fault tolerance** and additional opportunities for processing data locally (the design goal of “bringing the computation to the data”). \n",
    "\n",
    "HDFS blocks are stored and managed on a slave node (called **DataNode**) of the HDFS cluster.\n",
    "\n",
    "DataNodes are responsible for managing block storage and access for reading and writing of data, as well as for block replication, which is part of the data ingestion process\n",
    "\n",
    "<center>\n",
    "<img src = \"./assets/img/Captura_hadhoop_01.png\", height = 250>\n",
    "</center>\n",
    "\n",
    "The information about the filesystem and its virtual directories, files, and the physical blocks that comprise the files is stored in the filesystem metadata\n",
    "\n",
    "The *filesystem metadata* is stored in resident memory on the HDFS master node process known as the **NameNode**. \n",
    "\n",
    "The NameNode is responsible for providing HDFS clients with **block locations** for read and write operations, with which the clients communicate directly with the DataNodes for data operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src = \"./assets/img/Captura_hadhoop_03.png\", height = 300>\n",
    "</center>\n",
    "\n",
    "<center>\n",
    "<img src = \"./assets/img/Captura_hadhoop_04.png\", height = 300>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application Scheduling Using YARN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* YARN governs and orchestrates the processing of data in Hadoop. which usually is data *sourced from* and *written* to **HDFS**.\n",
    "\n",
    "* The YARN cluster architecture is a **master/slave cluster** framework like HDFS, with a master node called the *ResourceManager* and one or more slave node daemons called *NodeManagers* running on worker.\n",
    "\n",
    "* The *ResourceManager* is responsible for granting (giving or conceding) cluster *compute resources* to applications running on the cluster. \n",
    "\n",
    "* Resources are granted in units called *containers*, which are predefined combinations of *CPU cores* and *memory*.\n",
    "\n",
    "* Containers are used to isolate resources dedicated to a process or processes.\n",
    "\n",
    "* The ResourceManager also *tracks available capacity* on the cluster as applications finish and release their reserved resources, and it tracks the status of applications running on the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src = \"./assets/img/Captura_hadoop_05.png\", height = 300>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process pictured in Figure 1.6 works as follows:\n",
    "1. A client submits an application (like a *Pig*) to the ResourceManager.\n",
    "2. The ResourceManager allocates an ApplicationMaster process on a\n",
    "NodeManager with sufficient capacity to be assigned this role.\n",
    "3. The ApplicationMaster negotiates task containers with the\n",
    "ResourceManager to be run on NodeManagers—which can include the\n",
    "NodeManager on which the ApplicationMaster is running as well—and\n",
    "dispatches processing to the NodeManagers hosting the task containers for\n",
    "the application.\n",
    "4. The NodeManagers report their task attempt status and progress to the\n",
    "ApplicationMaster.\n",
    "5. The ApplicationMaster reports progress and the status of the application to\n",
    "the ResourceManager.\n",
    "6. The ResourceManager reports application progress, status, and results to the\n",
    "client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Apache Spark** is an open-source powerful distributed querying and processing engine\n",
    "\n",
    "* As an alternative to *MapReduce*, Spark implements a distributed, fault-tolerant, in-memory structure called a *Resilient Distributed Dataset* (RDD).\n",
    "\n",
    "* Spark is written in Scala, which is built on top of the Java Virtual Machine (JVM) and Java runtime.\n",
    "\n",
    "* Uses for Spark, `Spark` supports a wide range of applications, including the following:\n",
    "  * Extract-transform-load (ETL) operations\n",
    "  * Predictive analytics and machine learning\n",
    "  * Data access operations, such as SQL queries and visualizations \n",
    "  * Text mining and text processing.\n",
    "  * Real-time event processing\n",
    "  * Graph applications\n",
    "  * Pattern recognition\n",
    "  * Recommendation engines\n",
    "  \n",
    "* It provides flexibility and extensibility of MapReduce an API of Hadoop.\n",
    "\n",
    "* The Spark APIs are accessible (native) in *Java*, *Scala*, *Python*, *R* and *SQL*\n",
    "\n",
    "* Apache Spark can be used to build applications or package them up as libraries to be deployed on a cluster or perform quick analytics interactively through notebooks (like, for instance, Jupyter, Spark-Notebook, Databricks notebooks, and Apache Zeppelin).\n",
    "\n",
    "* Apache Spark  is a host of libraries familiar to *data analysts*, Spark Data Frames is similar to `data.frame` or `data.table` in Pandas.\n",
    "\n",
    "* Can be used SQL language.\n",
    "\n",
    "* Apache Spark are several already implemented and tuned algorithms, statistical models, and frameworks:\n",
    "    * MLlib and ML for machine learning.\n",
    "    * GraphX and GraphFrames for graph processing, \n",
    "    * and Spark Streaming (DStreams and Structured)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Apache Spark can easily run locally on a laptop.\n",
    "* yet can also easily be deployed in standalone mode, over YARN, or Apache Mesos - either on your local cluster or in the cloud.\n",
    "* It can read and write from a diverse data sources including (but not limited to) HDFS, Apache Cassandra, Apache HBase, and S3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src = \"./assets/img/Captura_spark_01.png\", height = 250>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Jobs and APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution process\n",
    "\n",
    "Any Spark application spins off a single driver process (that can contain multiple jobs) on the master node that then directs executor processes (that contain multiple tasks) distributed to a number of worker nodes as noted in the following diagram:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src = \"./assets/img/Captura_spark_02.png\", height = 250>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The driver process determines the number and the composition of the task processes directed to the executor nodes based on the graph generated for the given job.\n",
    "\n",
    "A Spark job is associated with a chain of object dependencies organized in a direct acyclic graph (DAG) such as the following example generated from the Spark UI. Given this, Spark can optimize the scheduling (for example, determine the number of tasks and workers required) and execution of these tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src = \"./assets/img/Captura_spark_03.png\", height = 250>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resilient Distributed Dataset\n",
    "\n",
    "* Apache Spark is built around a distributed collection of immutable Java Virtual Machine (JVM) objects called Resilient Distributed Datasets (RDDs for short).\n",
    "\n",
    "* The Spark RDD is the primary data abstraction structure for Spark applications. \n",
    "\n",
    "* Spark RDDs can be thought of as in-memory collections of data distributed across a cluster.\n",
    "\n",
    "* RDDs are calculated against *cached*, but stored *in-memory*.\n",
    "\n",
    "* It can load input data into an RDD, transforming the RDD into subsequent RDDs, and then storing or presenting the final output for an application from the resulting final RDD\n",
    "\n",
    "* These objects allow any job to perform calculations very quickly.\n",
    "\n",
    "* RDDs expose some transformations (such as `map(...)`, `reduce(...)`, and `filter(...)`).\n",
    "\n",
    "* RDDs *apply and log transformations* to the data in **parallel**, resulting in both increased speed and fault-tolerance.\n",
    "\n",
    "* RDDs provide *data lineage* - a form of an ancestry tree for each intermediate step in the form of a graph.\n",
    "\n",
    "    * Data lineage is the process of understanding, recording, and visualizing data as it flows from data sources to consumption. This includes all transformations the data underwent along the way—how the data was transformed, what changed, and why.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src = \"./assets/img/Captura_spark_04.png\", height = 300>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RDDs have two sets of parallel operations: *transformations* and *actions*.\n",
    "  * Transformation: map, flatmap, filter, distict, reduceByKey, mapPartitions\n",
    "  * Actions: collect, collectAsMap, reduce, countByKey, take, countByValue $\\;$\n",
    "* RDD transformation operations are lazy in a sense that they do not compute their results immediately. The transformations are only\n",
    "computed when an action is executed and the results need to be returned to the driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames\n",
    "\n",
    "* DataFrames, like RDDs, are immutable collections of data distributed among the nodes in a cluster.\n",
    "\n",
    "* Unlike RDDs, in DataFrames data is organized into named columns.\n",
    "\n",
    "### Datasets\n",
    "\n",
    "* The goal for datasets was to provide a type-safe, programming interface. This allowed developers to work with semi-structured data (like JSON or key-value pairs) with compile time type safety (that is, production applications can be checked for errors before they run). \n",
    "\n",
    "* Only available in Scala or Java. \n",
    "\n",
    "* The Datasets API contain high-level domain specific language operations such as sum(), avg(), join(), and group(). This latter trait means that you have the flexibility of traditional Spark RDDs but the code is also easier to express, read, and write\n",
    "\n",
    "### Project Tungsten\n",
    "\n",
    "* Tungsten is the codename for an umbrella project of Apache Spark's execution engine. The project focuses on improving the Spark algorithms so they use memory and CPU more efficiently, pushing the performance of modern hardware closer to its limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying a Multi-Node Spark Standalone Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying Spark in the Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can deploy Spark in the cloud. We can take the plataform, application and workload Spark to cloud "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Amazon Web Services\n",
    "* Google Cloud Platform\n",
    "* Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AWS portfolio contains dozens of different services from IaaS products such as Elastic Compute Cloud (EC2), storage services such as S3, and PaaS products such as Redshift.\n",
    "\n",
    "There two methods to create Spark cluster in AWS: EC2 and EMR.\n",
    "\n",
    "To use any of the AWS deployment options, you need a valid AWS account and API keys if you are using the AWS software\n",
    "development kit (SDK) or command line interface (CLI)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EC2\n",
    "\n",
    "Amazon Elastic Compute Cloud (EC2) is a part of Amazon.com's cloud-computing platform, Amazon Web Services (AWS), that allows users to rent virtual computers on which to run their own computer applications. \n",
    "\n",
    "EC2 encourages scalable deployment of applications by providing a web service through which a user can boot an Amazon Machine Image (AMI) to configure a virtual machine, which Amazon calls an \"instance\", containing any software desired. \n",
    "\n",
    "Mor information: \n",
    "* https://en.wikipedia.org/wiki/Amazon_Elastic_Compute_Cloud  \n",
    "* https://aws.amazon.com/ec2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Spark on EC2*\n",
    "\n",
    "There are several predeveloped Amazon Machine Images (AMIs) available in the AWS Marketplace; these have a pre-installed and configured release of Spark.\n",
    "\n",
    "You can also create Spark clusters on containers by using the EC2 Container Service. There are numerous options to create these, from existing projects available in GitHub and elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Spark Cluster Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The components of a ``Spark application`` are the `Driver`, the `Master`, the `Cluster Manager`, and the `Executor`(s), which run on `worker nodes`, or `Workers`.\n",
    "\n",
    "For the context of a Spark Standalone application\n",
    "\n",
    "<div align = \"center\">\n",
    "  <img src = \"./assets/img/spark-components.png\" height =  250>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd88188",
   "metadata": {},
   "source": [
    "All Spark components, including the Driver, Master, and Executor processes, run in Java virtual machines (JVMs)\n",
    "\n",
    "The instructions given by the client are compiled by `Scala` and run on JVM that is a runtime engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "7679c2132d3f6ce38c9df14d554b39c06862b36a4e6689c81f9ae15bd0911d7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
